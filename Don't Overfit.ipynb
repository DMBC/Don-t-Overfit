{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import sys, os\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score, r2_score, make_scorer\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n\n# some heuristic settings\nrfe_min_features = 12\nrfe_step = 15\nrfe_cv = 20\nsss_n_splits = 20\nsss_test_size = 0.35\ngrid_search_cv = 20\nnoise_std = 0.01\nr2_threshold = 0.185\nrandom_seed = 213\n\nnp.random.seed(random_seed)\n\n# import data\ntrain = pd.read_csv('../input/train.csv')\ntrain_y = train['target']\ntrain_X = train.drop(['id','target'], axis=1).values\n\ntest = pd.read_csv('../input/test.csv')\ntest = test.drop(['id'], axis=1).values\n\n# scale using RobustScaler\n# fitting scaler on full data outperforms fitting on test_X only (+0.006 kaggle score)\ndata = RobustScaler().fit_transform(np.concatenate((train_X, test), axis=0))\ntrain_X = data[:250]\ntest = data[250:]\n\n# add a bit of noise to train_X to reduce overfitting\ntrain_X += np.random.normal(0, noise_std, train_X.shape)\n\n# define roc_auc_metric robust to only one class in y_pred\ndef scoring_roc_auc(y, y_pred):\n    try:\n        return roc_auc_score(y, y_pred)\n    except:\n        return 0.5\n\nrobust_roc_auc = make_scorer(scoring_roc_auc)\n\n# define model and its parameters\nmodel = Lasso(alpha=0.031, tol=0.1, random_state=random_seed, selection='random')\n\nparam_grid = {\n            'alpha' : [0.022, 0.021, 0.02, 0.019, 0.023, 0.024, 0.025, 0.026, 0.027, 0.029, 0.031],\n            'tol'   : [0.0013, 0.0014, 0.001, 0.0015, 0.0011, 0.0012, 0.0016, 0.0017]\n        }\n\n# define recursive elimination feature selector\nfeature_selector = RFECV(model, min_features_to_select=rfe_min_features, scoring=robust_roc_auc, step=rfe_step, verbose=0, cv=rfe_cv, n_jobs=-1)\n\nprint(\"counter | val_mse  |  val_mae  |  val_roc  |  val_cos  |  val_dist  |  val_r2    | feature_count \")\nprint(\"-------------------------------------------------------------------------------------------------\")\n\npredictions = pd.DataFrame()\ncounter = 0\n# split training data to build one model on each traing-data-subset\nfor train_index, val_index in StratifiedShuffleSplit(n_splits=sss_n_splits, test_size=sss_test_size, random_state=random_seed).split(train_X, train_y):\n    X, val_X = train_X[train_index], train_X[val_index]\n    y, val_y = train_y[train_index], train_y[val_index]\n\n    # get the best features for this data set\n    feature_selector.fit(X, y)\n    # remove irrelevant features from X, val_X and test\n    X_important_features        = feature_selector.transform(X)\n    val_X_important_features    = feature_selector.transform(val_X)\n    test_important_features     = feature_selector.transform(test)\n\n    # run grid search to find the best Lasso parameters for this subset of training data and subset of features \n    grid_search = GridSearchCV(feature_selector.estimator_, param_grid=param_grid, verbose=0, n_jobs=-1, scoring=robust_roc_auc, cv=20)\n    grid_search.fit(X_important_features, y)\n\n    # score our fitted model on validation data\n    val_y_pred = grid_search.best_estimator_.predict(val_X_important_features)\n    val_mse = mean_squared_error(val_y, val_y_pred)\n    val_mae = mean_absolute_error(val_y, val_y_pred)\n    val_roc = roc_auc_score(val_y, val_y_pred)\n    val_cos = cosine_similarity(val_y.values.reshape(1, -1), val_y_pred.reshape(1, -1))[0][0]\n    val_dst = euclidean_distances(val_y.values.reshape(1, -1), val_y_pred.reshape(1, -1))[0][0]\n    val_r2  = r2_score(val_y, val_y_pred)\n\n    # if model did well on validation, save its prediction on test data, using only important features\n    # r2_threshold (0.185) is a heuristic threshold for r2 error\n    # you can use any other metric/metric combination that works for you\n    if val_r2 > r2_threshold:\n        message = '<-- OK'\n        prediction = grid_search.best_estimator_.predict(test_important_features)\n        predictions = pd.concat([predictions, pd.DataFrame(prediction)], axis=1)\n    else:\n        message = '<-- skipping'\n\n\n    print(\"{0:2}      | {1:.4f}   |  {2:.4f}   |  {3:.4f}   |  {4:.4f}   |  {5:.4f}    |  {6:.4f}    |  {7:3}         {8}  \".format(counter, val_mse, val_mae, val_roc, val_cos, val_dst, val_r2, feature_selector.n_features_, message))\n    \n    counter += 1\n\nprint(\"-------------------------------------------------------------------------------------------------\")\nprint(\"{}/{} models passed validation threshold and will be ensembled.\".format(len(predictions.columns), sss_n_splits))\n\nmean_pred = pd.DataFrame(predictions.mean(axis=1))\nmean_pred.index += 250\nmean_pred.columns = ['target']\nmean_pred.to_csv('submission.csv', index_label='id', index=True)        \n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}